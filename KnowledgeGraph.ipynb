{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need. How about you‚Äîhow are *you* doing today? üòä\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set up your Azure OpenAI credentials\n",
    "azure_endpoint = \"your az endpoint\"\n",
    "api_key = \"key\"\n",
    "api_version = \"2024-10-21\"\n",
    "deployment_name = \"gpt-4o\"  # Your deployment name\n",
    "\n",
    "# Create the OpenAI client\n",
    "client = openai.AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version\n",
    ")\n",
    "\n",
    "# Make the API call\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,  # Use 'model' instead of 'engine'\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import networkx as nx\n",
    "import openai\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def extract_code_structure(file_path, G, current, total, method_count):\n",
    "    \"\"\"Parses a Python file and extracts classes, methods, and relationships into a DAG.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"[INFO] Processing file {current}/{total}: {file_path}\")\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tree = ast.parse(f.read())\n",
    "\n",
    "    parent_stack = []\n",
    "    \n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.ClassDef):\n",
    "            code_snippet = ast.unparse(node)\n",
    "            G.add_node(node.name, type=\"class\", metadata=code_snippet, file=file_path)\n",
    "            parent_stack.append(node.name)\n",
    "            \n",
    "            for base in node.bases:\n",
    "                if isinstance(base, ast.Name):  \n",
    "                    G.add_edge(base.id, node.name, relation=\"inherits\")\n",
    "        \n",
    "        elif isinstance(node, ast.FunctionDef):\n",
    "            if parent_stack:\n",
    "                class_name = parent_stack[-1]\n",
    "                code_snippet = ast.unparse(node)\n",
    "                G.add_node(node.name, type=\"method\", metadata=code_snippet, file=file_path)\n",
    "                G.add_edge(class_name, node.name, relation=\"contains\")\n",
    "                method_count.append(node.name)\n",
    "        \n",
    "        elif isinstance(node, ast.Call):\n",
    "            caller = parent_stack[-1] if parent_stack else None\n",
    "            \n",
    "            if hasattr(node.func, 'id'):\n",
    "                callee = node.func.id\n",
    "            elif hasattr(node.func, 'attr') and hasattr(node.func, 'value') and isinstance(node.func.value, ast.Name):\n",
    "                callee = f\"{node.func.value.id}.{node.func.attr}\"\n",
    "            else:\n",
    "                callee = None\n",
    "\n",
    "            if caller and callee:\n",
    "                G.add_edge(caller, callee, relation=\"calls\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[INFO] Completed {file_path} in {elapsed_time:.2f} seconds.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_gpt4o(batch_methods):\n",
    "    \"\"\"Uses GPT-4o to analyze a batch of code snippets.\"\"\"\n",
    "    if not batch_methods:\n",
    "        return {}\n",
    "\n",
    "    print(f\"[INFO] Sending batch of {len(batch_methods)} methods to GPT-4o for analysis...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    batch_code_snippets = \"\\n\\n\".join([\n",
    "        f\"Method: {name}\\nCode:\\n{code}\" for name, code in batch_methods.items()\n",
    "    ])\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following batch of functions and summarize their purposes:\n",
    "    {batch_code_snippets}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Updated for GPT-4o\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[INFO] GPT-4o analysis for batch completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    response_text = response.choices[0].message.content.split(\"\\n\\n\")\n",
    "    analysis_results = {name: response_text[i] if i < len(response_text) else \"No response\"\n",
    "                        for i, name in enumerate(batch_methods.keys())}\n",
    "    \n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_graph_with_llm(G, batch_size=15):\n",
    "    \"\"\"Parallel processing for LLM-based analysis with batching.\"\"\"\n",
    "    print(\"[INFO] Starting GPT-4o analysis for extracted code components...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_methods = len([node for node in G.nodes if G.nodes[node].get(\"type\") == \"method\"])\n",
    "    print(f\"[INFO] Total methods to analyze: {total_methods}\")\n",
    "\n",
    "    methods = {node: G.nodes[node].get(\"metadata\", \"\") for node in G.nodes if G.nodes[node].get(\"type\") == \"method\"}\n",
    "    method_names = list(methods.keys())\n",
    "\n",
    "    # Create batches of methods\n",
    "    batches = [method_names[i:i + batch_size] for i in range(0, len(method_names), batch_size)]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_batch = {\n",
    "            executor.submit(analyze_with_gpt4o, {name: methods[name] for name in batch}): batch\n",
    "            for batch in batches\n",
    "        }\n",
    "        \n",
    "        for future in future_to_batch:\n",
    "            batch = future_to_batch[future]\n",
    "            results = future.result()\n",
    "            for name in batch:\n",
    "                G.nodes[name][\"metadata\"] = results.get(name, \"Analysis unavailable.\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[INFO] GPT-4o analysis phase completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_documentation(graph, output_path=\"knowledge_graph_documentation.json\"):\n",
    "    \"\"\"Generates a detailed documentation from the knowledge graph.\"\"\"\n",
    "    documentation = {\n",
    "        \"metadata\": {\n",
    "            \"description\": \"Knowledge graph representation of a Python codebase.\",\n",
    "            \"total_nodes\": len(graph.nodes),\n",
    "            \"total_edges\": len(graph.edges),\n",
    "        },\n",
    "        \"nodes\": [],\n",
    "        \"edges\": []\n",
    "    }\n",
    "\n",
    "    for node, attributes in graph.nodes(data=True):\n",
    "        documentation[\"nodes\"].append({\n",
    "            \"name\": node,\n",
    "            \"type\": attributes.get(\"type\", \"unknown\"),\n",
    "            \"metadata\": attributes.get(\"metadata\", \"No metadata available\"),\n",
    "            \"file\": attributes.get(\"file\", \"Unknown file\")\n",
    "        })\n",
    "\n",
    "    for source, target, attributes in graph.edges(data=True):\n",
    "        documentation[\"edges\"].append({\n",
    "            \"source\": source,\n",
    "            \"target\": target,\n",
    "            \"relation\": attributes.get(\"relation\", \"unknown relation\")\n",
    "        })\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(documentation, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Documentation saved as '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_graph(directory):\n",
    "    \"\"\"Process all Python files in a directory with progress tracking.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"[INFO] Scanning directory for Python files...\")\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    method_count = []\n",
    "    \n",
    "    python_files = [os.path.join(root, file) for root, _, files in os.walk(directory) for file in files if file.endswith(\".py\")]\n",
    "    total_files = len(python_files)\n",
    "    print(f\"[INFO] Found {total_files} Python files.\")\n",
    "    \n",
    "    for index, file_path in enumerate(python_files, start=1):\n",
    "        G = extract_code_structure(file_path, G, index, total_files, method_count)\n",
    "    \n",
    "    print(f\"[INFO] Total methods found: {len(method_count)}\")\n",
    "    print(\"[INFO] Starting GPT-4o-based processing of extracted code...\")\n",
    "    \n",
    "    process_graph_with_llm(G)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"[INFO] Knowledge graph construction completed in {elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting knowledge graph generation...\n",
      "[INFO] Scanning directory for Python files...\n",
      "[INFO] Found 8 Python files.\n",
      "[INFO] Processing file 1/8: C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\constants.py\n",
      "[INFO] Completed C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\constants.py in 0.02 seconds.\n",
      "[INFO] Processing file 2/8: C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\Streamer.py\n",
      "[INFO] Completed C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\Streamer.py in 0.02 seconds.\n",
      "[INFO] Processing file 3/8: C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\StreamViewer.py\n",
      "[INFO] Completed C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\StreamViewer.py in 0.02 seconds.\n",
      "[INFO] Processing file 4/8: C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\test_local_streaming.py\n",
      "[INFO] Completed C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\test_local_streaming.py in 0.02 seconds.\n",
      "[INFO] Processing file 5/8: C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\utils.py\n",
      "[INFO] Completed C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\utils.py in 0.02 seconds.\n",
      "[INFO] Processing file 6/8: C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\camera\\Camera.py\n",
      "[INFO] Completed C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\camera\\Camera.py in 0.01 seconds.\n",
      "[INFO] Processing file 7/8: C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\camera\\test_camera.py\n",
      "[INFO] Completed C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\camera\\test_camera.py in 0.02 seconds.\n",
      "[INFO] Processing file 8/8: C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\camera\\__init__.py\n",
      "[INFO] Completed C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\camera\\__init__.py in 0.00 seconds.\n",
      "[INFO] Total methods found: 23\n",
      "[INFO] Starting GPT-4o-based processing of extracted code...\n",
      "[INFO] Starting GPT-4o analysis for extracted code components...\n",
      "[INFO] Total methods to analyze: 15\n",
      "[INFO] Sending batch of 15 methods to GPT-4o for analysis...\n",
      "[INFO] GPT-4o analysis for batch completed in 16.72 seconds.\n",
      "[INFO] GPT-4o analysis phase completed in 16.72 seconds.\n",
      "[INFO] Knowledge graph construction completed in 16.85 seconds.\n",
      "[INFO] Saving knowledge graph to file...\n",
      "‚úÖ Documentation saved as 'knowledge_graph_documentation.json'\n",
      "‚úÖ Knowledge graph saved successfully as 'knowledge_graph_large.gpickle'.\n",
      "‚è±Ô∏è Total execution time: 16.87 seconds.\n"
     ]
    }
   ],
   "source": [
    "# **Modify this path to point to your larger codebase**\n",
    "codebase_path = r\"C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\"\n",
    "\n",
    "# **Generate Knowledge Graph**\n",
    "print(\"[INFO] Starting knowledge graph generation...\")\n",
    "global_start_time = time.time()\n",
    "graph = build_knowledge_graph(codebase_path)\n",
    "\n",
    "# **Save Graph Using Pickle**\n",
    "print(\"[INFO] Saving knowledge graph to file...\")\n",
    "with open(\"knowledge_graph_large.gpickle\", \"wb\") as f:\n",
    "    pickle.dump(graph, f)\n",
    "\n",
    "# **Generate Documentation**\n",
    "generate_documentation(graph)\n",
    "\n",
    "total_execution_time = time.time() - global_start_time\n",
    "print(f\"‚úÖ Knowledge graph saved successfully as 'knowledge_graph_large.gpickle'.\")\n",
    "print(f\"‚è±Ô∏è Total execution time: {total_execution_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Knowledge graph loaded successfully!\n",
      "\n",
      "üìå **Nodes in Graph (First 10):**\n",
      "üîπ Streamer - {'type': 'class', 'metadata': 'class Streamer:\\n\\n    def __init__(self, server_address=SERVER_ADDRESS, port=PORT):\\n        \"\"\"\\n        Tries to connect to the StreamViewer with supplied server_address and creates a socket for future use.\\n\\n        :param server_address: Address of the computer on which the StreamViewer is running, default is `localhost`\\n        :param port: Port which will be used for sending the stream\\n        \"\"\"\\n        print(\\'Connecting to \\', server_address, \\'at\\', port)\\n        context = zmq.Context()\\n        self.footage_socket = context.socket(zmq.PUB)\\n        self.footage_socket.connect(\\'tcp://\\' + server_address + \\':\\' + port)\\n        self.keep_running = True\\n\\n    def start(self):\\n        \"\"\"\\n        Starts sending the stream to the Viewer.\\n        Creates a camera, takes a image frame converts the frame to string and sends the string across the network\\n        :return: None\\n        \"\"\"\\n        print(\\'Streaming Started...\\')\\n        camera = Camera()\\n        camera.start_capture()\\n        self.keep_running = True\\n        while self.footage_socket and self.keep_running:\\n            try:\\n                frame = camera.current_frame.read()\\n                image_as_string = image_to_string(frame)\\n                self.footage_socket.send(image_as_string)\\n            except KeyboardInterrupt:\\n                cv2.destroyAllWindows()\\n                break\\n        print(\\'Streaming Stopped!\\')\\n        cv2.destroyAllWindows()\\n\\n    def stop(self):\\n        \"\"\"\\n        Sets \\'keep_running\\' to False to stop the running loop if running.\\n        :return: None\\n        \"\"\"\\n        self.keep_running = False', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\Streamer.py'}\n",
      "üîπ main - {'type': 'method', 'metadata': 'The provided batch of functions can be grouped into three main categories based on their purposes: **streaming functionality**, **camera control**, and **unit testing**. Below is a detailed summary of each category and its corresponding methods:', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\StreamViewer.py'}\n",
      "üîπ __init__ - {'type': 'method', 'metadata': '---', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\Camera.py'}\n",
      "üîπ start - {'type': 'method', 'metadata': '### **1. Streaming Functionality**\\nThese methods are responsible for managing and processing video streams, either by sending or receiving frames over a network connection.', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\Streamer.py'}\n",
      "üîπ stop - {'type': 'method', 'metadata': '- **`main`**:\\n  - Serves as the entry point for the program.\\n  - Sets up a port for the streaming viewer, either using a default value or a user-provided argument.\\n  - Initializes a `StreamViewer` object and starts receiving the stream.', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\StreamViewer.py'}\n",
      "üîπ argparse.ArgumentParser - {}\n",
      "üîπ parser.add_argument - {}\n",
      "üîπ parser.parse_args - {}\n",
      "üîπ streamer.start - {}\n",
      "üîπ print - {}\n",
      "üîπ zmq.Context - {}\n",
      "üîπ context.socket - {}\n",
      "üîπ Camera - {'type': 'class', 'metadata': \"class Camera:\\n\\n    def __init__(self, height=RESOLUTION_H, width=RESOLUTION_W):\\n        self.current_frame = None\\n        self.height = height\\n        self.width = width\\n        self.camera = None\\n\\n    def start_capture(self, height=None, width=None, usingPiCamera=IS_RASPBERRY_PI):\\n        import imutils\\n        from imutils.video import VideoStream\\n        resolution = (self.height, self.width)\\n        if height:\\n            if width:\\n                resolution = (height, width)\\n        cf = VideoStream(usePiCamera=usingPiCamera, resolution=resolution, framerate=32).start()\\n        self.current_frame = cf\\n        time.sleep(2)\\n        if not usingPiCamera:\\n            frame = imutils.resize(self.current_frame.read(), width=resolution[0])\\n\\n    def stop_capture(self):\\n        print('Stopping Capture')\\n        self.current_frame.stop()\\n\\n    def capture_image(self):\\n        import cv2\\n        ramp_frames = 30\\n        self.camera = cv2.VideoCapture(CAMERA_PORT)\\n        _, im = self.camera.read()\\n        [self.camera.read() for _ in range(ramp_frames)]\\n        print('Taking image...')\\n        _, camera_capture = self.camera.read()\\n        del self.camera\\n        return camera_capture\\n\\n    def __del__(self):\\n        try:\\n            self.current_frame.release()\\n        except AttributeError:\\n            pass\", 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\Camera.py'}\n",
      "üîπ camera.start_capture - {}\n",
      "üîπ cv2.destroyAllWindows - {}\n",
      "üîπ image_to_string - {}\n",
      "üîπ StreamViewer - {'type': 'class', 'metadata': 'class StreamViewer:\\n\\n    def __init__(self, port=PORT):\\n        \"\"\"\\n        Binds the computer to a ip address and starts listening for incoming streams.\\n\\n        :param port: Port which is used for streaming\\n        \"\"\"\\n        context = zmq.Context()\\n        self.footage_socket = context.socket(zmq.SUB)\\n        self.footage_socket.bind(\\'tcp://*:\\' + port)\\n        self.footage_socket.setsockopt_string(zmq.SUBSCRIBE, np.unicode(\\'\\'))\\n        self.current_frame = None\\n        self.keep_running = True\\n\\n    def receive_stream(self, display=True):\\n        \"\"\"\\n        Displays displayed stream in a window if no arguments are passed.\\n        Keeps updating the \\'current_frame\\' attribute with the most recent frame, this can be accessed using \\'self.current_frame\\'\\n        :param display: boolean, If False no stream output will be displayed.\\n        :return: None\\n        \"\"\"\\n        self.keep_running = True\\n        while self.footage_socket and self.keep_running:\\n            try:\\n                frame = self.footage_socket.recv_string()\\n                self.current_frame = string_to_image(frame)\\n                if display:\\n                    cv2.imshow(\\'Stream\\', self.current_frame)\\n                    cv2.waitKey(1)\\n            except KeyboardInterrupt:\\n                cv2.destroyAllWindows()\\n                break\\n        print(\\'Streaming Stopped!\\')\\n\\n    def stop(self):\\n        \"\"\"\\n        Sets \\'keep_running\\' to False to stop the running loop if running.\\n        :return: None\\n        \"\"\"\\n        self.keep_running = False', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\StreamViewer.py'}\n",
      "üîπ receive_stream - {'type': 'method', 'metadata': '- **`receive_stream`**:\\n  - Receives video frames from a socket connection and updates the `current_frame` attribute with the most recent frame.\\n  - Optionally displays the stream in a window using OpenCV (`cv2.imshow`).\\n  - Stops streaming gracefully when interrupted.', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\StreamViewer.py'}\n",
      "üîπ stream_viewer.receive_stream - {}\n",
      "üîπ np.unicode - {}\n",
      "üîπ string_to_image - {}\n",
      "üîπ cv2.imshow - {}\n",
      "üîπ cv2.waitKey - {}\n",
      "üîπ TestLocalStreaming - {'type': 'class', 'metadata': 'class TestLocalStreaming(unittest.TestCase):\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        super(TestLocalStreaming, cls).setUpClass()\\n        cls.stream_viewer = StreamViewer()\\n        Thread(target=lambda: cls.stream_viewer.receive_stream(display=False)).start()\\n        cls.streamer = Streamer()\\n        Thread(target=lambda: cls.streamer.start()).start()\\n        time.sleep(5)\\n\\n    def test_camera_image(self):\\n        self.assertIsInstance(self.stream_viewer.current_frame, numpy.ndarray)\\n\\n    def test_camera_image_not_null(self):\\n        self.assertIsNotNone(self.stream_viewer.current_frame)\\n\\n    @classmethod\\n    def tearDownClass(cls):\\n        super(TestLocalStreaming, cls).tearDownClass()\\n        cls.streamer.stop()\\n        cls.stream_viewer.stop()', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\test_local_streaming.py'}\n",
      "üîπ setUpClass - {'type': 'method', 'metadata': '- **`start`**:\\n  - Initiates the streaming process by creating a `Camera` object and capturing frames.\\n  - Converts captured frames into a string format and sends them across the network through a socket.\\n  - Continuously streams until interrupted or stopped.', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\test_camera.py'}\n",
      "üîπ test_camera_image - {'type': 'method', 'metadata': '- **`stop`**:\\n  - Stops the streaming loop by setting the `keep_running` flag to `False`.', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\test_camera.py'}\n",
      "üîπ test_camera_image_not_null - {'type': 'method', 'metadata': '---', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\test_camera.py'}\n",
      "üîπ tearDownClass - {'type': 'method', 'metadata': '### **2. Camera Control**\\nThese methods manage the camera hardware, handle frame capture, and clean up resources.', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\test_camera.py'}\n",
      "üîπ unittest.main - {}\n",
      "üîπ time.sleep - {}\n",
      "üîπ self.assertIsInstance - {}\n",
      "üîπ self.assertIsNotNone - {}\n",
      "üîπ super - {}\n",
      "üîπ Thread - {}\n",
      "üîπ start_capture - {'type': 'method', 'metadata': '- **`__init__`**:\\n  - Initializes a camera-related object with default resolution settings (`height` and `width`).\\n  - Sets up attributes like `current_frame` and `camera`.', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\Camera.py'}\n",
      "üîπ stop_capture - {'type': 'method', 'metadata': \"- **`start_capture`**:\\n  - Starts capturing video frames using the `VideoStream` class (from the `imutils` library).\\n  - Configures the camera's resolution and framerate.\\n  - Handles Raspberry Pi-specific camera usage (`usingPiCamera`).\", 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\Camera.py'}\n",
      "üîπ capture_image - {'type': 'method', 'metadata': '- **`stop_capture`**:\\n  - Stops the camera capture process and releases resources.', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\Camera.py'}\n",
      "üîπ __del__ - {'type': 'method', 'metadata': '- **`capture_image`**:\\n  - Captures a single image from the camera after discarding several \"ramp frames\" for stabilization.\\n  - Returns the captured image as a NumPy array.', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\Camera.py'}\n",
      "üîπ camera.capture_image - {}\n",
      "üîπ preview_image - {}\n",
      "üîπ cv2.VideoCapture - {}\n",
      "üîπ imutils.resize - {}\n",
      "üîπ VideoStream - {}\n",
      "üîπ range - {}\n",
      "üîπ TestCameraImage - {'type': 'class', 'metadata': 'class TestCameraImage(unittest.TestCase):\\n\\n    def test_camera_image(self):\\n        self.assertIsInstance(Camera().capture_image(), numpy.ndarray)\\n\\n    def test_camera_image_not_null(self):\\n        self.assertIsNotNone(Camera().capture_image())', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\test_camera.py'}\n",
      "üîπ TestCameraStream - {'type': 'class', 'metadata': 'class TestCameraStream(unittest.TestCase):\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        super(TestCameraStream, cls).setUpClass()\\n        cls.camera = Camera()\\n        cls.camera.start_capture()\\n\\n    def test_camera_stream(self):\\n        self.assertIsInstance(self.camera.current_frame.read(), numpy.ndarray)\\n\\n    def test_camera_stream_not_null(self):\\n        self.assertIsNotNone(self.camera.current_frame.read())\\n\\n    @classmethod\\n    def tearDownClass(cls):\\n        super(TestCameraStream, cls).tearDownClass()\\n        cls.camera.stop_capture()', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\test_camera.py'}\n",
      "üîπ test_camera_stream - {'type': 'method', 'metadata': \"- **`__del__`**:\\n  - Releases the `current_frame` resource when the object is deleted.\\n  - Ensures proper cleanup in case the attribute doesn't exist.\", 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\test_camera.py'}\n",
      "üîπ test_camera_stream_not_null - {'type': 'method', 'metadata': '---', 'file': 'C:\\\\Users\\\\rajrishi\\\\OneDrive - Microsoft\\\\Desktop\\\\SmoothStream-master\\\\camera\\\\test_camera.py'}\n",
      "\n",
      "üìå **Edges in Graph (First 10):**\n",
      "üî∏ Streamer ‚Üí main ({'relation': 'calls'})\n",
      "üî∏ Streamer ‚Üí __init__ ({'relation': 'contains'})\n",
      "üî∏ Streamer ‚Üí start ({'relation': 'contains'})\n",
      "üî∏ Streamer ‚Üí stop ({'relation': 'contains'})\n",
      "üî∏ Streamer ‚Üí argparse.ArgumentParser ({'relation': 'calls'})\n",
      "üî∏ Streamer ‚Üí parser.add_argument ({'relation': 'calls'})\n",
      "üî∏ Streamer ‚Üí parser.parse_args ({'relation': 'calls'})\n",
      "üî∏ Streamer ‚Üí Streamer ({'relation': 'calls'})\n",
      "üî∏ Streamer ‚Üí streamer.start ({'relation': 'calls'})\n",
      "üî∏ Streamer ‚Üí print ({'relation': 'calls'})\n",
      "üî∏ Streamer ‚Üí zmq.Context ({'relation': 'calls'})\n",
      "üî∏ Streamer ‚Üí context.socket ({'relation': 'calls'})\n",
      "üî∏ Streamer ‚Üí Camera ({'relation': 'calls'})\n",
      "üî∏ Streamer ‚Üí camera.start_capture ({'relation': 'calls'})\n",
      "üî∏ Streamer ‚Üí cv2.destroyAllWindows ({'relation': 'calls'})\n",
      "üî∏ Streamer ‚Üí image_to_string ({'relation': 'calls'})\n",
      "üî∏ Camera ‚Üí __init__ ({'relation': 'contains'})\n",
      "üî∏ Camera ‚Üí start_capture ({'relation': 'contains'})\n",
      "üî∏ Camera ‚Üí stop_capture ({'relation': 'contains'})\n",
      "üî∏ Camera ‚Üí capture_image ({'relation': 'contains'})\n",
      "üî∏ Camera ‚Üí __del__ ({'relation': 'contains'})\n",
      "üî∏ Camera ‚Üí Camera ({'relation': 'calls'})\n",
      "üî∏ Camera ‚Üí camera.capture_image ({'relation': 'calls'})\n",
      "üî∏ Camera ‚Üí preview_image ({'relation': 'calls'})\n",
      "üî∏ Camera ‚Üí camera.start_capture ({'relation': 'calls'})\n",
      "üî∏ Camera ‚Üí time.sleep ({'relation': 'calls'})\n",
      "üî∏ Camera ‚Üí print ({'relation': 'calls'})\n",
      "üî∏ Camera ‚Üí cv2.VideoCapture ({'relation': 'calls'})\n",
      "üî∏ Camera ‚Üí cv2.imshow ({'relation': 'calls'})\n",
      "üî∏ Camera ‚Üí cv2.waitKey ({'relation': 'calls'})\n",
      "üî∏ Camera ‚Üí imutils.resize ({'relation': 'calls'})\n",
      "üî∏ Camera ‚Üí VideoStream ({'relation': 'calls'})\n",
      "üî∏ Camera ‚Üí range ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí main ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí __init__ ({'relation': 'contains'})\n",
      "üî∏ StreamViewer ‚Üí receive_stream ({'relation': 'contains'})\n",
      "üî∏ StreamViewer ‚Üí stop ({'relation': 'contains'})\n",
      "üî∏ StreamViewer ‚Üí argparse.ArgumentParser ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí parser.add_argument ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí parser.parse_args ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí StreamViewer ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí stream_viewer.receive_stream ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí zmq.Context ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí context.socket ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí print ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí np.unicode ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí string_to_image ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí cv2.imshow ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí cv2.waitKey ({'relation': 'calls'})\n",
      "üî∏ StreamViewer ‚Üí cv2.destroyAllWindows ({'relation': 'calls'})\n",
      "\n",
      "üí° **LLM Response:**\n",
      "The codebase represents a system for real-time video streaming, which includes components for capturing video frames, sending them over a network, receiving them, and displaying them. It is organized into several Python files, each serving a specific purpose. Here's an explanation of the key components:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Overview of the Codebase**\n",
      "The codebase is divided into three main categories:\n",
      "1. **Streaming Functionality**: Handles sending and receiving video streams over a network.\n",
      "2. **Camera Control**: Manages camera hardware to capture video frames and images.\n",
      "3. **Unit Testing**: Provides tests to ensure the functionality of camera operations and streaming processes.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Key Components**\n",
      "\n",
      "#### **Streamer**\n",
      "- **Location**: `Streamer.py`\n",
      "- **Purpose**: Sends video frames captured by a camera to a remote viewer using ZeroMQ sockets.\n",
      "- **Methods**:\n",
      "  - `__init__`: Initializes the streamer by connecting to a server and creating a socket for communication.\n",
      "  - `start`: Captures video frames from the camera, converts them to strings, and sends them over the network.\n",
      "  - `stop`: Stops the streaming process by setting a flag (`keep_running`) to `False`.\n",
      "- **Dependencies**:\n",
      "  - Uses the `Camera` class to capture video frames.\n",
      "  - Utilizes the `image_to_string` function to convert frames into a format suitable for network transmission.\n",
      "  - Relies on ZeroMQ (`zmq`) for socket communication.\n",
      "\n",
      "#### **Camera**\n",
      "- **Location**: `camera/Camera.py`\n",
      "- **Purpose**: Manages the camera hardware and provides methods for capturing video frames and images.\n",
      "- **Methods**:\n",
      "  - `__init__`: Initializes camera settings like resolution and attributes (`current_frame` and `camera`).\n",
      "  - `start_capture`: Starts capturing video frames using `imutils` and `VideoStream`. Handles Raspberry Pi-specific configurations.\n",
      "  - `stop_capture`: Stops the video capture process and releases resources.\n",
      "  - `capture_image`: Captures a single image after stabilizing the camera by discarding several initial frames.\n",
      "  - `__del__`: Cleans up resources when the object is deleted.\n",
      "- **Dependencies**:\n",
      "  - Uses OpenCV (`cv2`) for image capture and display.\n",
      "  - Relies on `imutils` for resizing frames and handling video streams.\n",
      "\n",
      "#### **StreamViewer**\n",
      "- **Location**: `StreamViewer.py`\n",
      "- **Purpose**: Receives video frames sent by the `Streamer` and optionally displays them in a window.\n",
      "- **Methods**:\n",
      "  - `__init__`: Sets up a ZeroMQ socket to bind to a port and listen for incoming streams.\n",
      "  - `receive_stream`: Continuously receives video frames, converts them back to images, and optionally displays them using OpenCV.\n",
      "  - `stop`: Stops the receiving loop by setting a flag (`keep_running`) to `False`.\n",
      "- **Dependencies**:\n",
      "  - Uses `string_to_image` to convert received frame strings back to image format.\n",
      "  - Utilizes OpenCV (`cv2`) for displaying the video stream.\n",
      "\n",
      "#### **Unit Testing**\n",
      "- **Location**: `test_local_streaming.py` and `camera/test_camera.py`\n",
      "- **Purpose**: Provides unit tests to validate the functionality of the streaming system and camera operations.\n",
      "- **Classes**:\n",
      "  - `TestLocalStreaming`: Tests the integration of `Streamer` and `StreamViewer`. Validates that frames are being sent and received correctly.\n",
      "    - `setUpClass`: Sets up the streamer and viewer in separate threads for testing.\n",
      "    - `test_camera_image`: Ensures that the received frame is a valid NumPy array.\n",
      "    - `test_camera_image_not_null`: Ensures the received frame is not `None`.\n",
      "    - `tearDownClass`: Stops the streamer and viewer.\n",
      "  - `TestCameraImage`: Tests the `Camera` class's ability to capture images.\n",
      "    - `test_camera_image`: Validates that captured images are NumPy arrays.\n",
      "    - `test_camera_image_not_null`: Ensures captured images are not `None`.\n",
      "  - `TestCameraStream`: Tests the `Camera` class's ability to stream video frames.\n",
      "    - `setUpClass`: Initializes the camera and starts capturing.\n",
      "    - `test_camera_stream`: Validates that streamed frames are NumPy arrays.\n",
      "    - `test_camera_stream_not_null`: Ensures streamed frames are not `None`.\n",
      "    - `tearDownClass`: Stops the camera capture.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Dependencies**\n",
      "The codebase relies on several third-party libraries and tools:\n",
      "- **ZeroMQ (`zmq`)**: Handles socket communication for sending and receiving video streams.\n",
      "- **OpenCV (`cv2`)**: Provides tools for image capture, processing, and display.\n",
      "- **Imutils**: Simplifies video stream handling and image resizing.\n",
      "- **NumPy**: Processes image data as arrays.\n",
      "- **Threading**: Runs the streamer and viewer in parallel during testing.\n",
      "- **UnitTest (`unittest`)**: Validates the system through test cases.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Workflow**\n",
      "1. **Streaming**:\n",
      "   - The `Streamer` captures frames using the `Camera` class, converts them to strings, and sends them to the `StreamViewer`.\n",
      "   - The `StreamViewer` receives the frames, converts them back to images, and optionally displays them in a window.\n",
      "\n",
      "2. **Camera Control**:\n",
      "   - The `Camera` class manages the hardware, captures video frames, and provides methods for capturing images and cleaning up resources.\n",
      "\n",
      "3. **Testing**:\n",
      "   - Unit tests ensure that the streaming workflow and camera operations are functioning correctly.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Execution**\n",
      "- **Main Entry Points**:\n",
      "  - The `main` function in `StreamViewer.py` initializes the `StreamViewer` and starts receiving streams.\n",
      "  - The `Streamer` can be started independently to send video streams.\n",
      "- **Testing**:\n",
      "  - Run the test files (`test_local_streaming.py` and `camera/test_camera.py`) to validate the system.\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Summary**\n",
      "The codebase is designed for real-time video streaming and viewing. It uses ZeroMQ for network communication, OpenCV for image processing, and Imutils for camera handling. The system is modular, with separate components for streaming, camera control, and testing, ensuring flexibility and maintainability.\n"
     ]
    }
   ],
   "source": [
    "def load_graph(file_path):\n",
    "    \"\"\"Loads the knowledge graph from a .gpickle file.\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def ask_llm_about_graph(graph, query):\n",
    "    \"\"\"Uses GPT-4o to analyze the knowledge graph and respond to queries.\"\"\"\n",
    "    nodes_info = []\n",
    "    edges_info = []\n",
    "\n",
    "    for node, attributes in graph.nodes(data=True):\n",
    "        metadata = attributes.get(\"metadata\", \"No metadata available\")\n",
    "        file_path = attributes.get(\"file\", \"Unknown file\")\n",
    "        nodes_info.append(f\"{node} (File: {file_path}) - {metadata}\")\n",
    "\n",
    "    for source, target, attributes in graph.edges(data=True):\n",
    "        relation = attributes.get(\"relation\", \"unknown relation\")\n",
    "        edges_info.append(f\"{source} ‚Üí {target} ({relation})\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are given a knowledge graph representing a Python codebase.\n",
    "\n",
    "    **Nodes in the Graph:**  \n",
    "    {\"\\n\".join(nodes_info[:50])}  # Limiting to first 50 to avoid exceeding token limits\n",
    "\n",
    "    **Edges in the Graph:**  \n",
    "    {\"\\n\".join(edges_info[:50])}  # Limiting to first 50 to avoid exceeding token limits\n",
    "\n",
    "    **User Query:** \"{query}\"\n",
    "    \n",
    "    Based on the knowledge graph, provide an answer.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Using GPT-4o model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    graph_file = \"knowledge_graph_large.gpickle\"\n",
    "\n",
    "    try:\n",
    "        graph = load_graph(graph_file)\n",
    "        print(\"‚úÖ Knowledge graph loaded successfully!\")\n",
    "\n",
    "        print(\"\\nüìå **Nodes in Graph (First 10):**\")\n",
    "        for node, data in list(graph.nodes(data=True))[:50]:\n",
    "            print(f\"üîπ {node} - {data}\")\n",
    "\n",
    "        print(\"\\nüìå **Edges in Graph (First 10):**\")\n",
    "        for source, target, data in list(graph.edges(data=True))[:50]:\n",
    "            print(f\"üî∏ {source} ‚Üí {target} ({data})\")\n",
    "\n",
    "        query = input(\"\\nüîç Enter your query about the code: \")\n",
    "        response = ask_llm_about_graph(graph, query)\n",
    "\n",
    "        print(\"\\nüí° **LLM Response:**\")\n",
    "        print(response)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Graph file '{graph_file}' not found. Run the extraction script first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a cosmosDB client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting azure-cosmos\n",
      "  Downloading azure_cosmos-4.9.0-py3-none-any.whl.metadata (80 kB)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in c:\\users\\rajrishi\\appdata\\roaming\\python\\python312\\site-packages (from azure-cosmos) (1.32.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\rajrishi\\appdata\\roaming\\python\\python312\\site-packages (from azure-cosmos) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\rajrishi\\appdata\\roaming\\python\\python312\\site-packages (from azure-core>=1.30.0->azure-cosmos) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\rajrishi\\appdata\\roaming\\python\\python312\\site-packages (from azure-core>=1.30.0->azure-cosmos) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rajrishi\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-cosmos) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rajrishi\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-cosmos) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rajrishi\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-cosmos) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajrishi\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-cosmos) (2025.1.31)\n",
      "Downloading azure_cosmos-4.9.0-py3-none-any.whl (303 kB)\n",
      "Installing collected packages: azure-cosmos\n",
      "Successfully installed azure-cosmos-4.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-cosmos\n",
    "\n",
    "from azure.core.exceptions import AzureError\n",
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "ENDPOINT = \"https://knowledgebase.documents.azure.com:443/\"\n",
    "\n",
    "# Uncomment the following lines if you want to use Azure AD authentication\n",
    "#credential = DefaultAzureCredential()\n",
    "#client = CosmosClient(ENDPOINT, credential)\n",
    "\n",
    "# Connect to the Cosmos DB account using the connection string\n",
    "dbclient = CosmosClient.from_connection_string(\"AccountEndpoint=https://knowledgebase.documents.azure.com:443/;AccountKey=oH6ZTvZtobQ5frdb4exjk25d1TD9HozSAtyFmlDSe6DNxciwU1k4mYgweCiMOrReP8FRHzXxIC79ACDbZ11YHA==;\")\n",
    "\n",
    "# Create database (if not exists)\n",
    "database = dbclient.create_database_if_not_exists(id=\"knowledgebase\")\n",
    "\n",
    "# Create container (if not exists)\n",
    "vector_embedding_policy = { \n",
    "    \"vectorEmbeddings\": [ \n",
    "        { \n",
    "            \"path\": \"/embedding\", \n",
    "            \"dataType\": \"float32\", \n",
    "            \"distanceFunction\": \"cosine\", \n",
    "            \"dimensions\": 10 \n",
    "        } \n",
    "    ]    \n",
    "}\n",
    "indexing_policy = { \n",
    "    \"includedPaths\": [ \n",
    "        { \n",
    "            \"path\": \"/*\" \n",
    "        } \n",
    "    ], \n",
    "    \"excludedPaths\": [ \n",
    "        { \n",
    "            \"path\": \"/\\\"_etag\\\"/?\"\n",
    "        },\n",
    "        { \n",
    "            \"path\": \"/embedding/*\"\n",
    "        }\n",
    "    ], \n",
    "    \"vectorIndexes\": [ \n",
    "        {\n",
    "            \"path\": \"/embedding\", \n",
    "            \"type\": \"quantizedFlat\" \n",
    "        },\n",
    "    ] \n",
    "}\n",
    "\n",
    "container = database.create_container_if_not_exists(\n",
    "    id=\"VectorStore\",\n",
    "    partition_key=PartitionKey(path=\"/file\",\n",
    "                               indexing_policy=indexing_policy,\n",
    "                               vector_embedding_policy=vector_embedding_policy)  # Adjust based on your partition strategy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk text and create embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Number of chunks: 31\n",
      "üìù First chunk: # **Detailed Documentation for the Code Structure** This documentation provides an in-depth explanation of the code structure and functionality for the `Streamer` class, its methods, and associated components. The `Streamer` class is part of a streaming application designed to capture video frames from a camera, convert them into a string format, and transmit them over a network to a viewer. --- ## **File Information** - **File Path:** `C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\SmoothStream-master\\Streamer.py` - **Purpose:** Implements the `Streamer` class to handle video streaming functionality. --- ## **Class: Streamer** ### **Overview** The `Streamer` class is responsible for: 1. Establishing a network connection to a viewer application (`StreamViewer`). 2. Capturing video frames from a camera. 3. Sending the video frames over the network in a serialized format. --- ### **Constructor: `__init__`** #### **Definition** ```python def __init__(self, server_address=SERVER_ADDRESS, port=PORT):\n",
      "üîπ First embedding generated successfully!\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "embeddings_client = AzureOpenAI(\n",
    "    api_key=\"key\",\n",
    "    azure_endpoint=\"endpoint\",\n",
    "    api_version=\"2024-10-21\",\n",
    ")\n",
    "\n",
    "def split_text(text, max_length=1000, min_length=500):\n",
    "    \"\"\"Splits text into chunks ensuring each chunk is within max and min word limits.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(' '.join(current_chunk)) >= max_length:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk and len(' '.join(current_chunk)) >= min_length:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def create_embeddings(text_chunk, model=\"text-embedding-ada-002\"):\n",
    "    \"\"\"Generates embeddings for a given text chunk.\"\"\"\n",
    "    response = embeddings_client.embeddings.create(input=text_chunk, model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Read the detailed documentation file\n",
    "with open(r\"C:\\Users\\rajrishi\\OneDrive - Microsoft\\Desktop\\detailed_documentation.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "# Split the text into manageable chunks\n",
    "chunks = split_text(input_text, max_length=1000, min_length=500)\n",
    "print(f\"‚úÖ Number of chunks: {len(chunks)}\")\n",
    "print(f\"üìù First chunk: {chunks[0]}\")\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = [(create_embeddings(chunk), chunk) for chunk in chunks]\n",
    "print(f\"üîπ First embedding generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the embeddings to Cosmos DB\n",
    "for embedding, chunk in embeddings:\n",
    "    try:\n",
    "        # Create a new document in the container\n",
    "        container.create_item(\n",
    "            body={\n",
    "                'id': str(hash(chunk)),  # Unique ID for the document\n",
    "                'embedding': embedding,\n",
    "                'text': chunk,\n",
    "                'file': \"file1\"  # Example partition key value\n",
    "            },\n",
    "        )\n",
    "    except AzureError as e:\n",
    "        print(f\"Error creating item: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from azure.core.exceptions import AzureError\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# OpenAI Configuration\n",
    "deployment_name = \"gpt-4o\"  # Ensure this matches your deployment\n",
    "api_version = \"2024-10-21\"\n",
    "\n",
    "# Initialize OpenAI Client\n",
    "embeddings_client = AzureOpenAI(\n",
    "    api_key=\"key\",\n",
    "    azure_endpoint=\"azep\",\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "def get_nearest_vectors(collection, query_vector, k=5):\n",
    "    \"\"\"Retrieve top-k most relevant documentation snippets based on vector similarity.\"\"\"\n",
    "    query = '''\n",
    "        SELECT TOP @k c.text\n",
    "        FROM c \n",
    "        WHERE VectorDistance(c.embedding, @embedding) < 1\n",
    "    '''\n",
    "    parameters = [\n",
    "        {\"name\": \"@embedding\", \"value\": query_vector},\n",
    "        {\"name\": \"@k\", \"value\": k}\n",
    "    ]\n",
    "\n",
    "    results = collection.query_items(query=query, parameters=parameters, enable_cross_partition_query=True)\n",
    "\n",
    "    relevant_docs = []\n",
    "    for result in results:\n",
    "        relevant_docs.append(result[\"text\"])\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "def chatbot():\n",
    "    \"\"\"Interactive chatbot loop.\"\"\"\n",
    "    while True:\n",
    "        user_input = input(\"\\nAsk your query (or type 'exit' to quit): \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        query_vector = create_embeddings(user_input)\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = get_nearest_vectors(container, query_vector, k=5)\n",
    "        if not relevant_docs:\n",
    "            print(\"I couldn't find relevant information in the knowledge base.\")\n",
    "            continue\n",
    "\n",
    "        # Prepare context\n",
    "        context = \"\\n\".join(relevant_docs)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant helping with AI-related queries.\"},\n",
    "            {\"role\": \"system\", \"content\": f\"Here are some relevant snippets from the knowledge base:\\n{context}\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "\n",
    "        # Stream response from OpenAI\n",
    "        response_stream = embeddings_client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            temperature=0.7,\n",
    "            max_tokens=800,\n",
    "            messages=messages,\n",
    "            stream=True  # Enable streaming\n",
    "        )\n",
    "\n",
    "        print(\"\\nResponse:\")\n",
    "        for chunk in response_stream:\n",
    "            if chunk.choices and chunk.choices[0].delta and \"content\" in chunk.choices[0].delta:\n",
    "                print(chunk.choices[0].delta[\"content\"], end=\"\", flush=True)  # Print each token as it arrives\n",
    "        print()  # Ensure the final output ends with a newline\n",
    "\n",
    "# Start the chatbot loop\n",
    "chatbot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
